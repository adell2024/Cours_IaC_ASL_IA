Play with kubectl

Get info about the cluster :

kubectl cluster-info

Get the nodes :

kubectl get nodes

With labels :

kubectl get nodes --show-labels

Create a namespace :

kubectl create namespace devoxxfr

Set the context to this namespace :

kubectl config set-context --current --namespaceinsa23fr
Your first deployment

Monitor the events :

watch kubectl get events --sort-by=.metadata.creationTimestamp

Create deployment :

kubectl create deployment insafr23 --image toto/insa23:1.0

Display pods :

kubectl get pods

with labels :

kubectl get pods --show-labels

Get into the pod :

kubectl exec -it pod-id -- /bin/bash

Inside the pod :

curl localhost:8080/hello

Describe pod :

kubectl describe pod pod-id

Delete a pod :

kubectl delete pod pod-id

Scale deployment :

kubectl scale deployment insafr23 --replicas 3
Expose your deployment

kubectl expose deployment insafr23 --port 8080 --type=LoadBalancer

Get service :

kubectl get service

If public IP is available : curl public_ip:8080/hello
Roll-out new image

kubectl set image deployment/insafr23 devoxxfr23fr23=toto/devoxxfr23:2.0
Apply manifests

Create new namespace :

kubectl create deployment devoxxfr-manifests

kubectl config set-context --current --namespace=devoxxfr-manifests
Apply deployment manifest

kubectl apply -f manifests/insafr23-v1-deployment.yml

Edit deployment :

kubectl edit deployment/insafr23

Look for replicas , set it to 2 , save and exit.
Apply Service manifest

kubectl apply -f manifests/insafr23-service.yml
Requests and Limits

Setting just the cpu request (very high) and check pod going in pending status :

kubectl apply -f manifests/insafr23-v1-deployment-10cpus.yml

Setting requests and limits :

kubectl apply -f manifests/insafr23-v1-deployment-limits.yml
Health probes

kubectl apply -f manifests/insafr23-v1-deployment-health.yml

Go into the pod :

kubectl exec -it pod-id -- /bin/bash

Force the readiness probe to fail :

curl localhost:8080/health/misbehave

Observe pod's status.

Make pod ready again :

curl localhost:8080/health/behave

Make pod a zombie and make liveness check fail :

curl localhost:8080/health/shoot

Observice pod getting restarted.
Playing with service selector

Deploy a second version of the deployment :

kubectl apply manifests/insa23-v2-deployment-health.yml

2 new pods should appear (replicas is set to 2 for v2)

Edit service :

kubectl edit svc/insa23

Look for selector section is set it to :

selector:
    app.kubernetes.io/name: insa23fr23-v2
    app.kubernetes.io/version: "2.0"

Save and exit.
ConfigMap

kubectl set env deployment/insa23-v2 GREETING=hola

kubectl create cm my-config --from-env-file=my.properties

kubectl apply -f manifests/insa23fr23-v1-deployment-configmap
Secret

kubectl create secret generic mysecret --from-literal=user='MyUserName' --from-literal=password='mypassword'

kubectl get secret mysecret -o yaml

kubectl replace -f manifests/insa23-v1-deployment-secrets.yml
Taint

kubectl describe nodes | egrep "Name:|Taints:"

kubectl taint nodes --all=true color=blue:NoSchedule

kubectl taint node devnation-m02 color:NoSchedule- 

kubectl taint nodes --all=true color=blue:NoSchedule --overwrite

kubectl label nodes devnation-m02 color=blue 
